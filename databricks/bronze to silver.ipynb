{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a1a0ab-0c6c-4a9c-a6f3-b1fadc0e49d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1806e76c-4d65-4929-bccd-9b2349c0b397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store schema and table names\n",
    "schema_tables = {}\n",
    "\n",
    "# Loop through each schema folder in the specified path\n",
    "for schema in dbutils.fs.ls('/mnt/bronze/'):\n",
    "    schema_name = schema.name.split('/')[0]\n",
    "    \n",
    "    # Initialize a list to hold table names for the current schema\n",
    "    tables = []\n",
    "    \n",
    "    # Loop through each table folder inside the current schema folder\n",
    "    for table in dbutils.fs.ls(f'/mnt/bronze/{schema_name}'):\n",
    "        table_name = table.name.split('/')[0]\n",
    "        tables.append(table_name)\n",
    "    \n",
    "    # Add the schema and its tables to the dictionary\n",
    "    schema_tables[schema_name] = tables\n",
    "\n",
    "# Display the schema_tables dictionary\n",
    "schema_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa33cd1-eb10-4ca8-ab9b-8bdb43b7161f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for schema, tables in schema_tables.items():\n",
    "    for table in tables:\n",
    "        path = f'/mnt/bronze/{schema}/{table}/{table}.parquet'\n",
    "        df = spark.read.format('parquet').load(path)\n",
    "        column = df.columns\n",
    "\n",
    "        for col in column:\n",
    "            if \"date\" in col or \"Date\" in col:\n",
    "                df = df.withColumn(col, date_format(from_utc_timestamp(df[col].cast(TimestampType()), \"UTC\"),\"yyyy-MM-dd\"))\n",
    "\n",
    "        df = df.dropDuplicates()\n",
    "\n",
    "        output_path = f'/mnt/silver/{schema}/{table}/'\n",
    "        df.write.format('delta').mode('overwrite').save(output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze to silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
